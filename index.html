<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SORNet: Spatial Object-Centric Representations for Sequential Manipulation</title>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120436611-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-120436611-3');
    </script>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>

    <link rel="StyleSheet" href="style.css" type="text/css"/>
  </head>

  <body>
    <br>
    <center>
      <h1 class="title"><b>SORNet</b>: Spatial Object-Centric Representations for Sequential Manipulation</h1>
    </center>

    <br>
    <div class="container">
      <table align=center width=100%>
        <tr>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://wentaoyuan.github.io">Wentao Yuan</a><sup>1</sup></span>
            </center>
          </td>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://cpaxton.github.io">Chris Paxton</a><sup>2</sup></span>
            </center>
          </td>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://karthikdesingh.com">Karthik Desingh</a><sup>1</sup></span>
            </center>
          </td>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://homes.cs.washington.edu/~fox">Dieter Fox</a><sup>1,2</sup></span>
            </center>
          </td>
        </tr>
      </table>

      <table align=center width=100%>
        <tr>
          <td align=center width=50%>
            <center>
              <span class="text"><sup>1</sup>University of Washington</span>
            </center>
          </td>
          <td align=center width=50%>
            <center>
              <span class="text"><sup>2</sup>NVIDIA</span>
            </center>
          </td>
        </tr>
      </table>
    </div>
    <br>

    <div class="container grey_container">
      <center><h2 class="sectitle">Overview Video</h2></center>
        <div class="container video_container" align=center>
          <iframe width="800" height="450" src="https://www.youtube.com/embed/MdqeLXrDpME" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
    </div>

    <div class="container">
      <center><h2 class="sectitle">Abstract</h2></center>
      <p class="text">
        Sequential manipulation tasks require a robot to perceive the state of an environment and plan a sequence of actions leading to a desired goal state, where the ability to reason about spatial relationships among object entities from raw sensor inputs is crucial. Prior works relying on explicit state estimation or end-to-end learning struggle with novel objects. 
      </p>
      <p class="text">
        In this work, we propose <b>SORNet</b> (<b>S</b>patial <b>O</b>bject-Centric <b>R</b>epresentation <b>Net</b>work), which extracts object-centric representations from RGB images conditioned on canonical views of the objects of interest. We show that the object embeddings learned by SORNet generalize <i>zero-shot</i> to <i>unseen</i> object entities on three spatial reasoning tasks: spatial relationship classification, skill precondition classification and relative direction regression, significantly outperforming baselines. Further, we present real-world robotic experiments demonstrating the usage of the learned object embeddings in task planning for sequential manipulation.
      </p>
    </div>

    <div class="container grey_container">
      <center><h2 class="sectitle">Spatial Object Centric Network</h2></center>
      <p class="text">
        Via a series of attention layers, SORNet extracts an object-centric embedding vector for each query object conditioned on a canonical view of the object. The embedding vector captures information about the object's spatial relationship with other entities in the environment. Note that the number of appearance of query objects need not to match with the number and appearance of objects in the input frame.
      </p>
      <div class="container widget_container">
        <img width="100%" src="images/network_architecture.png"/>
      </div>
    </div>

    <div class="container">
      <center><h2 class="sectitle">Downstream Tasks</h2></center>
      <p class="text">
        The object-centric embedding produced by SORNet enables zero-shot generalization to unseen objects on a variety of downstream tasks, including predicting spatial relationships, classifying skill preconditions and regressing relative direction from the end-effector to the object center.
      </p>
      <div class="container widget_container">
        <img width="100%" src="images/CLEVR_valB_004000.png"/>
      </div>
      <center><p class="caption">Spatial Relationship Prediction on <a href="https://cs.stanford.edu/people/jcjohns/clevr">CLEVR-CoGenT</a></p></center>
      <div class="container widget_container">
        <img width="100%" src="images/image000003.png"/>
      </div>
      <center><p class="caption">Skill Precondition Classification in a real-world tabletop manipulation scene</p></center>
      <div class="container widget_container">
        <img width="30%" src="images/3302_0_ee_green_block.gif"/>
        <img width="30%" src="images/5377_0_ee_green_block.gif"/>
        <img width="30%" src="images/3302_1_ee_green_block.gif"/>
      </div>
      <center><p class="caption">Visuo Servoing using predicted 3D direction from the end-effector to the object center</p></center>
    </div>

    <div class="container grey_container"><td>
      <center><h2 class="sectitle">Paper & Code</h2></center>
      <table align=center width=60%>
        <tr>
          <td align=center width=50%>
            <a href="https://arxiv.org/abs/2109.03891">
              <img class="layered-paper-big" width=40% src="images/page1.png"/>
              <!-- <p class="text">Paper<br>(arXiv)</p> -->
            </a>
          </td>
          <td align=center width=50%>
            <a href="https://github.com/wentaoyuan">
              <img width=50% src="images/github.png"/>
              <!-- <p class="text">Code<br>(coming soon)</p> -->
            </a>
          </td>
        </tr>
      </table>
    </div>

    <div class="container">
      <center><h2 class="sectitle">Citation</h2></center>
      <div class="text citation">
        @article{yuan2021sornet,<br>
          &nbsp; &nbsp; title={SORNet: Spatial Object-Centric Representations for Sequential Manipulation},<br>
          &nbsp; &nbsp; author={Yuan, Wentao and Paxton, Chris and Desingh, Karthik and Fox, Dieter},<br>
          &nbsp; &nbsp; journal={arXiv preprint arXiv:2109.03891},<br>
          &nbsp; &nbsp; year={2021}<br>
        }
      </div>
    </div>

  </body>
</html>
