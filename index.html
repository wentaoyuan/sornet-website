<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>SORNet: Spatial Object-Centric Representations for Sequential Manipulation</title>

    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-120436611-3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'UA-120436611-3');
    </script>
    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript">google.load("jquery", "1.3.2");</script>

    <link rel="StyleSheet" href="style.css" type="text/css"/>
  </head>

  <body>
    <br>
    <center>
      <h1 class="suptitle">SORNet</h1>
      <h1 class="title">Spatial Object-Centric Representations for Sequential Manipulation</h1>
    </center>

    <br>
    <div class="container">
      <table align=center width=100%>
        <tr>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://wentaoyuan.github.io">Wentao Yuan</a><sup>1</sup></span>
            </center>
          </td>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://cpaxton.github.io">Chris Paxton</a><sup>2</sup></span>
            </center>
          </td>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://karthikdesingh.com">Karthik Desingh</a><sup>1</sup></span>
            </center>
          </td>
          <td align=center width=25%>
            <center>
              <span class="text"><a href="https://homes.cs.washington.edu/~fox">Dieter Fox</a><sup>1,2</sup></span>
            </center>
          </td>
        </tr>
      </table>

      <table align=center width=100%>
        <tr>
          <td align=center width=50%>
            <center>
              <span class="text"><sup>1</sup>University of Washington</span>
            </center>
          </td>
          <td align=center width=50%>
            <center>
              <span class="text"><sup>2</sup>NVIDIA</span>
            </center>
          </td>
        </tr>
      </table>
    </div>
    <br>

    <div class="container">
      <center><h2 class="sectitle">Overview</h2></center>
        <p class="text">
          Sequential manipulation tasks require a robot to constantly reason about spatial relationships among entities in the scene. Prior works relying on explicit state estimation or end-to-end learning struggle with novel objects or novel tasks. Thus, we propose <b>SORNet</b> (<b>S</b>patial <b>O</b>bject-Centric <b>R</b>epresentation <b>Net</b>work), which enables <i>zero-shot</i> generalization to <i>unseen</i> objects on spatial reasoning tasks.
        </p>
        <div class="container widget_container">
          <iframe width=100% height="500" src="https://www.youtube.com/embed/MdqeLXrDpME" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
    </div>

    <div class="container grey_container">
      <center><h2 class="sectitle">Spatial Object Centric Network</h2></center>
      <p class="text">
        SORNet consists of two parts. The embedding network extracts object-centric embeddings from RGB images conditioned on canonical views of the objects of interest. The redaout network takes the embedding vectors and predicts discrete or continuous spatial relations among entities in the scene. Note that the object queries (canonical object views) can be captured in scenarios different from the input image (e.g. with different lighting and camera view).
      </p>
      <div class="container widget_container">
        <img width="100%" src="images/architecture.png"/>
      </div>
    </div>

    <div class="container">
      <center><h2 class="sectitle">Downstream Tasks</h2></center>
      <p class="text">
        The object-centric embedding produced by SORNet enables zero-shot generalization to unseen objects on a variety of downstream tasks, including predicting spatial relationships, classifying skill preconditions and regressing relative direction from the end-effector to the object center.
      </p>
      <div class="container widget_container">
        <img width="100%" src="images/CLEVR.png"/>
      </div>
      <center><p class="caption">Spatial Relationship Prediction on <a href="https://cs.stanford.edu/people/jcjohns/clevr">CLEVR-CoGenT</a></p></center>
      <div class="container widget_container">
        <img width="100%" src="images/real_world.png"/>
      </div>
      <center><p class="caption">Skill Precondition Classification in a real-world tabletop manipulation scene</p></center>
      <div class="container widget_container">
        <img width="50%" src="images/visual_servoing.gif"/>
      </div>
      <center><p class="caption">Visual Servoing using predicted 3D direction from the end-effector to the object center</p></center>
    </div>

    <div class="container grey_container"><td>
      <center><h2 class="sectitle">Paper & Code</h2></center>
      <table align=center width=60%>
        <tr>
          <td align=center width=50%>
            <a href="https://arxiv.org/abs/2109.03891">
              <img class="layered-paper-big" width=40% src="images/page1.png"/>
            </a>
          </td>
          <td align=center width=50%>
            <a href="https://github.com/wentaoyuan/sornet">
              <img width=50% src="images/github.png"/>
            </a>
          </td>
        </tr>
      </table>
    </div>

    <div class="container">
      <center><h2 class="sectitle">Citation</h2></center>
      <div class="text citation">
        @inproceedings{yuan2021sornet,<br>
          &nbsp; &nbsp; title        = {SORNet: Spatial Object-Centric Representations for Sequential Manipulation},<br>
          &nbsp; &nbsp; author       = {Wentao Yuan and Chris Paxton and Karthik Desingh and Dieter Fox},<br>
          &nbsp; &nbsp; booktitle    = {5th Annual Conference on Robot Learning},<br>
          &nbsp; &nbsp; pages        = {148--157},<br>
          &nbsp; &nbsp; year         = {2021},<br>
          &nbsp; &nbsp; organization = {PMLR}<br>
        }
      </div>
    </div>

  </body>
</html>
